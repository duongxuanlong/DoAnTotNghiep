\chapter{Phương pháp đề xuất}
\label{Chapter3}

%Thuật toán gom nhóm phân cấp được thể hiện thông qua sử dụng ma trận tương đồng.
%Điều này đòi hỏi dung lượng cần thiết để lưu trữ ma trận tương đồng là $\frac{1}{2} m^2$ (giả định ma trận tương đồng là ma trận vuông) với $m$ là số điểm dữ liệu.
%Thuật toán cũng cần khoảng trống cần thiết để giữ cho việc đánh dấu tỷ lệ các nhóm được gom với tổng số nhóm, có giá trị là $m - 1$ và trừ đi những nhóm đơn lẻ.
%Vì thế, dung lượng cần thiết để chạy thuật toán gom nhóm phân cấp là $O(m^2)$.
%
%Phân tích cơ bản dành cho thuật toán gom nhóm phân cấp cũng liên quan trực tiếp đến độ phức tạp tính toán.
%$O(m^2)$ là thời gian cần thiết để tính ma trận tương đồng.
%Sau bước đó, dựa vào thuật toán \ref{agl:agglomerative}, ta còn $m - 1$ lần lặp cho bước 3 và 4 vì ta có $m$ nhóm lúc ban đầu và mối lần lặp thì có 2 nhóm được gom vào.
%Nếu ta thực thi như tìm kiếm tuyến tính của ma trận tương đồng thì sau lần lặp thứ $i$ thì thời gian ở bước 3 sẽ là $O(m - i + 1)^2)$.
%Điều này tỷ lệ với số lương hiện tại của bình phương nhóm.%
%Thời gian cần để chạy bước thứ 4 là $O(m - i + 1)$ để cập nhật ma trận tương đồng sau khi gom 2 nhóm (một nhóm được gom lại chỉ mất khoảng $O(m - i + 1)$).
%Nếu như không có thay đổi, thuật toán có độ phức tạp là $O(m^3)$.
%Trong trường hợp khoảng cách từ nhóm này đến các nhóm khác được lưu trữ theo thứ tự thì có khả năng làm giảm quá trình tìm kiếm 2 nhóm gần nhất.
%Tuy nhiên, trường hợp tổng quát cho độ phức tạp của thuật toán là $O(m^2 \log)m$.

\section{Thể hiện văn bản}
\label{sec:thvb}
Như đã phân tích trong \ref{sec:dpt}, thuật toán gom nhóm phân cấp kết hợp có độ phức tạp khá cao và tốn nhiều dung lượng lúc thực thi.
Đồ án tập trung vào nhiệm vụ cải thiện dung lượng của thuật toán lúc thực thi để có thể mở rộng giới hạn dữ liệu để gom nhóm và tăng tốc thời gian thực thi.
Việc thuật toán tốn nhiều dung lượng để chạy thực nghiệm một phần đến từ biểu diễn văn bản bằng vector có số chiều quá lớn.
Chính vì vector có số chiều quá lớn nên không gian để lưu trữ cũng phải tăng theo.
Giả sử ngữ liệu có $m$ văn bản, số lượng từ vựng là $n$ thì dung lượng cần thiết để biểu diễn toàn bộ ngữ liệu dưới dạng vector sẽ là $m * n$.
Trong công thức trên, ta không thể thay đổi số lượng văn bản $m$ nên chỉ có thể cải thiện số chiều $n$ của vector để giảm bộ nhớ lúc thực thi.
Vì vậy, đồ án đã đề xuất thể hiện văn bản bằng doc2vec là biểu diễn vector có số chiều thấp hơn.

Việc văn bản được biểu diễn bằng vector là để giúp cho việc tính toán trở nên dễ dàng hơn do nội dung của văn bản đa phần bao gồm từ ngữ, câu chữ.
Nếu văn bản được để nguyên định dạng để gom nhóm thì gây ra nhiều khó khăn cho việc tính toán.
Vì vậy, văn bản sẽ được chuyển sang định dạng vector.
Tuy nhiên, việc biểu diễn văn bản bằng vector có nhiều cách khác nhau và cách biểu diễn này có thể ảnh hưởng đến hiệu năng của thuật toán nên phải chọn cách biểu diễn thích hợp với thuật toán gom nhóm phân cấp kết hợp.

Một trong số những cách biểu diễn văn bản bằng vector căn bản nhất là sử dụng vector nhị phân để xác định một từ có nằm trong vector hay không?
Cách biểu diễn này xây dựng tập hợp từ của các văn bản, số lượng từ trong tập hợp cũng chính là số chiều của vector.
Mỗi văn bản sẽ biểu diễn bằng vector theo thứ tự của tập hợp các từ và có giá trị 1 (từ này nằm trong văn bản) hoặc là 0 (từ này không nằm trong văn bản).
Ví dụ, ta có 2 văn bản như sau:

\textbf{VB1}: ``Tôi đi học''

\textbf{VB2}: ``Tôi đang đến trường''

Để biểu diễn vector nhị phân cho văn bản, trước hết ta sẽ xây dựng tập hợp từ thành bảng sau \ref{tab:3_1}:
\begin{table}[h!]
\centering
\caption{Bảng thể hiện vị trí của từ}
\label{tab:3_1}
\begin{tabular}{|c|c|}
\hline
Vị Trí & Từ							\\ \hline
0    & Tôi                \\ \hline
1    & đi               \\ \hline
2    & học               \\ \hline
3    & đang               \\ \hline
4    & đến               \\ \hline
5    & trường               \\ \hline
\end{tabular}
\end{table}

Vector nhị phân của 2 văn bản \textbf{VB1} và \textbf{VB2} là:

\textbf{VB1}: (1, 1, 1, 0, 0, 0)

\textbf{VB2}: (1, 0, 0, 1, 1, 1)

Điểm yếu của vector nhị phân là cách thể hiện này chỉ cho ta biết được từ nào có xuất hiện trong văn bản chứ không thể xác định được số lượng của một từ có trong văn bản là bao nhiêu.
Giả sử ta gộp \textbf{VB1} và \textbf{VB2} thành \textbf{VB3}.

\textbf{VB3}: ``Tôi đi học. Tôi đang đến trường''

Khi đó, vector nhị phân cho \textbf{VB3} sẽ là:

\textbf{VB3}: (1, 1, 1, 1, 1, 1)

Như vậy, vector nhị phân không thể cho ta biết được từ ``Tôi'' đã xuất hiện đến 2 lần trong \textbf{VB3}.
Vì vậy, một cách khác được đề xuất có tên gọi là TF-IDF (term frequency - inverse document frequency).
Theo như~\cite{tf-idf}: ``TF-IDF là tích của 2 số TF và IDF. Trong đó TF là tần số của một từ xuất hiện trong văn bản và IDF là tần số ngược của văn bản, nghĩa là số lần xuất hiện ở những văn bản khác nhau của từ đó''.
Công thức để tính TFIDF cho một từ trong văn bản là:

\begin{equation}
tfidf (t, d, D) = tf(t, d) * idf (t, D)
\end{equation}

\begin{equation}
idf (t, D) = log \frac{N}{|{d \in D: t \in d}|}
\end{equation}

\begin{enumerate}
\item[•]$D$: tập ngữ liệu
\item[•]$d$: văn bản thuộc tập $D$
\item[•]$N$: tổng số văn bản trong ngữ liệu $N = |D|$.
\item[•]$|{d \in D: t \in d}|$: số lượng văn bản mà từ $t$ xuất hiện. Tuy nhiên trong trường hợp từ nào đó không xuất hiện trong văn bản thì có thể dẫn đến chia cho 0 nên phần này sẽ được sửa lại thành  $1 + |{d \in D: t \in d}|$
\end{enumerate}

Chiều của vector theo cách biểu diễn TFIDF cũng chính là số lượng từ trong tập hợp từ.
Tuy nhiên, TFIDF có số chiều phụ thuộc vào số lượng từ trong tập hợp nên số chiều của vector là rất lớn.
Ngoài ra, dù vector có số chiều là rất lớn nhưng số lượng phần tử bằng 0 (từ này không xuất hiện trong văn bản) trong vector cũng nhiều không kém.
Điều này dẫn đến tốn kém quá nhiều dung lượng lưu trữ để biểu diễn cho vector TFIDF.
Vì vậy, một phương pháp biểu diễn khác để cải thiện vấn đề này, đó chính là doc2vec.
%Áp dụng cách biểu diễn TFIDF cho \textbf{VB1} và \textbf{VB2} thì ta có:
%
%\textbf{VB1}: ()
%
%\textbf{VB2}: ()

\section{Doc2vec}
Như đã đề cập trong phần thể hiện văn bản~\ref{sec:thvb}, vector biểu diễn văn bản thường có số chiều lớn nên gây ra nhiều bất cập trong quá trình gom nhóm văn bản.
Vì vậy, một phương pháp có tên gọi là doc2vec được đề xuất bởi nhóm tác giả Quoc V. Le và Tomas Mikolov~\cite{doc2vec-original} dùng để giải quyết những hạn chế của thể hiện văn bản.
Doc2vec là thuật toán học không giám sát để tạo ra vector thể hiện cho câu, đoạn văn hay là văn bản.
Mô hình doc2vec có nguồn gốc từ word2vec nên doc2vec còn có thể biết được ngữ cảnh của từ trong đoạn văn bản. 
Ví dụ, từ ``powerful'' và ``strong'' trong văn bản có nghĩa gần tương đồng với nhau so với từ ``Paris''.
Cho nên, đồ án sử dụng doc2vec để biểu diễn vector cho văn bản để có thể giúp cho gom nhóm phân cấp có kết quả tốt hơn.

Doc2vec sử dụng 2 mô hình khác nhau để huấn luyện.
Đó là mô hình distributed-memory (dm) tương ứng với mô hình contiuous-bag-of-words (cbow) trong word2vec và mô hình distributed-bag-of-words (dbow) tương ứng với mô hình skip-gram (sg) trong word2vec. 

\subsection{Mô hình distributed-memory (dm)}
Mô hình dm của doc2vec có nguồn gốc từ mô hình cbow của word2vec.
Mô hình này được phát triển dựa vào cách word2vec dự đoán từ theo ngữ cảnh.
Mặc dù vector của từ được khởi tạo ngẫu nhiên nhưng sau quá trình huấn luyện thì thuật toán cho ta biết được từ nào có thể xuất hiện tiếp theo dựa vào ngữ cảnh cho trước.
Đây là hướng tiếp cận được sử dụng cho mô hình dm của doc2vec.

Vì vậy, để có thể hiểu được mô hình dm của doc2vec, ta cần phải hiểu mô hình cbow của word2vec trước.
Như đã giới thiệu, mô hình cbow là kiến trúc để xây dựng vector thể hiện từ trong word2vec.
Word2vec là mạng nơ-ron với 3 lớp: lớp đầu vào, 1 lớp ẩn và lớp đầu ra.
Ý tưởng của cbow dựa vào những từ cho trước, thuật toán sẽ huấn luyện để có thể dự đoán từ có thể xuất hiện tiếp theo những từ này.
Lớp đầu vào là các từ ngữ cảnh liên quan đến từ đầu ra, còn lớp đầu ra là từ cần dự đoán.
Mô hình cbow được biểu diễn theo hình~\ref{pic:cbow}~\cite{word2vec-cbow}.

Mô hình~\ref{pic:cbow} là mạng nơ-ron có 3 lớp.
Trong đó, lớp đầu vào của mạng có số lượng các chữ là $C$ với mỗi chữ là vector được biểu diễn theo one-hot (từ nào xuất hiện thì giá trị tại vị trí đó của vector sẽ là 1, còn lại là 0 và vị trí này dựa vào cách xây dựng nên từ vựng có kích thước là $V$).
Lớp ẩn của mạng là vector $h$ có thước là $N$ (với $N$ được khởi tạo ngẫu nhiên).
Cuối cùng, lớp xuất của mạng là chữ $y$ cũng là vector được biểu diễn theo one-hot.
Các vector one-hot của lớp đầu vào kết nối với lớp ẩn thông qua ma trận $\textbf{W}$ có kích thước $V * N$ và lớp ẩn kết nối với lớp xuất qua ma trận $\textbf{W'}$ có kích thước là $N * V$.

Dựa vào mô hình~\ref{pic:cbow}, ta sẽ huấn luyện word2vec cho câu sau: ``The cat sat on the mat''.
Mục đích của word2vec là học cách thể hiện các chữ ``the'', ``cat'', ``sat'', ``on'', `` mat'' thành các vector.
Mạng nơ-ron sẽ học các đặc trưng (dựa vào trọng số $\textbf{W}$ và  $\textbf{W'}$) để có thể đoán ra được chữ ``on'' nếu ta nhập vào ``The cat sat''.
Khi ta nhập ``the'', ``cat'', ``sat'', quá trình huấn luyện sẽ điều chỉnh trọng số của mạng sao cho xác suất của chữ ``on'' là cao nhất ở lớp đầu ra.
Vì vậy, chữ ``on'' là kết quả được dự đoán và là chữ xuất hiện tiếp theo.
Ví dụ này được thể hiện qua hình~\ref{vd:cbow}~\cite{doc2vec-original}.

\begin{figure}[htp]
\psscalebox{1.0 1.0} % Change this value to rescale the drawing.
{
\begin{pspicture}(0,-2.92)(11.375,2.92)
\psdots[linecolor=black, dotstyle=square, dotsize=0.4, fillcolor=white](4.8,-0.52)
\psdots[linecolor=black, dotstyle=square, dotsize=0.4, fillcolor=white](5.2,-0.52)
\psdots[linecolor=black, dotstyle=square, dotsize=0.4, fillcolor=white](5.6,-0.52)
\psdots[linecolor=black, dotstyle=square, dotsize=0.4, fillcolor=white](6.0,-0.52)
\psdots[linecolor=black, dotstyle=square, dotsize=0.4, fillcolor=white](6.4,-0.52)
\psdots[linecolor=black, dotstyle=square, dotsize=0.4, fillcolor=white](7.2,-0.52)
\psdots[linecolor=black, dotstyle=square, dotsize=0.4, fillcolor=white](7.6,-0.52)
\psdots[linecolor=black, dotstyle=square, dotsize=0.4, fillcolor=white](8.0,-0.52)
\psdots[linecolor=black, dotstyle=square, dotsize=0.4, fillcolor=white](8.4,-0.52)
\psdots[linecolor=black, dotstyle=square, dotsize=0.4, fillcolor=white](8.8,-0.52)
\psdots[linecolor=black, dotstyle=square, dotsize=0.4, fillcolor=white](9.6,-0.52)
\psdots[linecolor=black, dotstyle=square, dotsize=0.4, fillcolor=white](10.0,-0.52)
\psdots[linecolor=black, dotstyle=square, dotsize=0.4, fillcolor=white](10.4,-0.52)
\psdots[linecolor=black, dotstyle=square, dotsize=0.4, fillcolor=white](10.8,-0.52)
\psdots[linecolor=black, dotstyle=square, dotsize=0.4, fillcolor=white](11.2,-0.52)
\psdots[linecolor=black, dotstyle=square, dotsize=0.4, fillcolor=white](6.8,1.08)
\psdots[linecolor=black, dotstyle=square, dotsize=0.4, fillcolor=white](7.2,1.08)
\psdots[linecolor=black, dotstyle=square, dotsize=0.4, fillcolor=white](7.6,1.08)
\psdots[linecolor=black, dotstyle=square, dotsize=0.4, fillcolor=white](8.0,1.08)
\rput[bl](5.2,-1.72){\textbf{W}}
\rput[bl](7.6,-1.72){\textbf{W}}
\rput[bl](10.0,-1.72){\textbf{W}}
\rput[bl](5.2,-2.92){the}
\rput[bl](7.6,-2.92){cat}
\rput[bl](10.0,-2.92){sat}
\rput[bl](7.2,2.68){on}
\psline[linecolor=black, linewidth=0.04, arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.0]{->}(5.6,-2.52)(5.6,-1.72)
\psline[linecolor=black, linewidth=0.04, arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.0]{->}(8.0,-2.52)(8.0,-1.72)
\psline[linecolor=black, linewidth=0.04, arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.0]{->}(10.4,-2.52)(10.4,-1.72)
\psline[linecolor=black, linewidth=0.04, arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.0]{->}(5.6,-1.32)(5.6,-0.92)
\psline[linecolor=black, linewidth=0.04, arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.0]{->}(8.0,-1.32)(8.0,-0.92)
\psline[linecolor=black, linewidth=0.04, arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.0]{->}(10.4,-1.32)(10.4,-0.92)
\psline[linecolor=black, linewidth=0.04, arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.0]{->}(5.6,-0.12)(6.8,0.68)
\psline[linecolor=black, linewidth=0.04, arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.0]{->}(8.0,-0.12)(7.6,0.68)
\psline[linecolor=black, linewidth=0.04, arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.0]{->}(10.4,-0.12)(8.4,0.68)
\psline[linecolor=black, linewidth=0.04, arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.0]{->}(7.2,1.48)(7.2,2.68)
\rput[bl](0.0,2.68){Classifier}
\rput[bl](0.0,1.08){Average/Concatenate}
\rput[bl](0.0,-1.72){World Matrix}
\end{pspicture}
}
\caption{Quá trình dự đoán từ dựa vào ngữ cảnh}
\label{vd:cbow}
\end{figure}

\begin{figure}[htp]
\makeatletter % For spaces in paths
\patchcmd\Gread@eps{\@inputcheck#1 }{\@inputcheck"#1"\relax}{}{}
\makeatother
\psscalebox{1.0 1.0} % Change this value to rescale the drawing.
{
\begin{pspicture}(0,-9.244781)(11.45,9.244781)
\pscircle[linecolor=black, linewidth=0.04, dimen=outer](2.057143,8.363352){0.4}
\pscircle[linecolor=black, linewidth=0.04, dimen=outer](2.057143,7.163352){0.4}
\psdots[linecolor=black, dotsize=0.2](2.057143,5.9633517)
\psdots[linecolor=black, dotsize=0.2](2.057143,5.163352)
\pscircle[linecolor=black, linewidth=0.04, dimen=outer](2.057143,4.363352){0.4}
\rput[bl](0.0,6.5347805){$x_{1k}$}
\pscircle[linecolor=black, linewidth=0.04, dimen=outer](2.057143,1.9633518){0.4}
\pscircle[linecolor=black, linewidth=0.04, dimen=outer](2.057143,0.76335186){0.4}
\psdots[linecolor=black, dotsize=0.2](2.057143,-0.43664813)
\psdots[linecolor=black, dotsize=0.2](2.057143,-1.2366481)
\pscircle[linecolor=black, linewidth=0.04, dimen=outer](2.057143,-2.036648){0.4}
\pscircle[linecolor=black, linewidth=0.04, dimen=outer](2.057143,-4.436648){0.4}
\pscircle[linecolor=black, linewidth=0.04, dimen=outer](2.057143,-5.636648){0.4}
\psdots[linecolor=black, dotsize=0.2](2.057143,-6.836648)
\psdots[linecolor=black, dotsize=0.2](2.057143,-7.636648)
\pscircle[linecolor=black, linewidth=0.04, dimen=outer](2.057143,-8.436648){0.4}
\psdots[linecolor=black, dotsize=0.2](1.2571429,-3.236648)
\psdots[linecolor=black, dotsize=0.2](1.2571429,-3.6366482)
\pscircle[linecolor=black, linewidth=0.04, dimen=outer](6.0571427,1.1633519){0.4}
\psdots[linecolor=black, dotsize=0.2](6.0571427,0.36335188)
\psdots[linecolor=black, dotsize=0.2](6.0571427,-0.43664813)
\pscircle[linecolor=black, linewidth=0.04, dimen=outer](6.0571427,-1.2366481){0.4}
\psframe[linecolor=black, linewidth=0.04, dimen=outer](2.4571428,9.163352)(1.6571429,3.5633519)
\psframe[linecolor=black, linewidth=0.04, dimen=outer](2.4571428,2.763352)(1.6571429,-2.8366482)
\psframe[linecolor=black, linewidth=0.04, dimen=outer](2.4571428,-3.6366482)(1.6571429,-9.236649)
\pscircle[linecolor=black, linewidth=0.04, dimen=outer](10.057143,1.9633518){0.4}
\pscircle[linecolor=black, linewidth=0.04, dimen=outer](10.057143,0.76335186){0.4}
\psdots[linecolor=black, dotsize=0.2](10.057143,-0.43664813)
\psdots[linecolor=black, dotsize=0.2](10.057143,-1.2366481)
\pscircle[linecolor=black, linewidth=0.04, dimen=outer](10.057143,-2.036648){0.4}
\psframe[linecolor=black, linewidth=0.04, dimen=outer](10.457143,2.763352)(9.657143,-2.8366482)
\psline[linecolor=black, linewidth=0.04](2.4571428,9.163352)(5.6571426,1.9633518)
\psframe[linecolor=black, linewidth=0.04, dimen=outer](6.457143,1.9633518)(5.6571426,-2.036648)
\psline[linecolor=black, linewidth=0.04](2.4571428,3.5633519)(5.6571426,-2.036648)
\psline[linecolor=black, linewidth=0.04](2.4571428,2.763352)(5.6571426,1.9633518)
\psline[linecolor=black, linewidth=0.04](2.4571428,-2.8366482)(5.6571426,-2.036648)
\psline[linecolor=black, linewidth=0.04](2.4571428,-3.6366482)(5.6571426,1.9633518)
\psline[linecolor=black, linewidth=0.04](2.4571428,-9.236649)(5.6571426,-2.036648)
\psline[linecolor=black, linewidth=0.04](6.457143,1.9633518)(9.657143,2.763352)
\psline[linecolor=black, linewidth=0.04](6.457143,-2.036648)(9.657143,-2.8366482)
\rput[bl](0.0,0.53478044){$x_{2k}$}
\rput[bl](0.0,-6.2652197){$x_{Ck}$}
\rput[bl](3.2,8.93478){Input layer}
\rput[bl](2.8,4.9347806){$\textbf{W}_{V*N}$}
\rput[bl](2.8,-0.26521954){$\textbf{W}_{V*N}$}
\rput[bl](2.8,-4.66522){$\textbf{W}_{V*N}$}
\rput[bl](3.2,-9.06522){$C*V-dim$}
\rput[bl](6.0,-3.0652196){$N-dim$}
\rput[bl](6.8,-0.26521954){$h_i$}
\rput[bl](8.0,0.13478045){$\textbf{W'}_{N*V}$}
\rput[bl](9.6,-3.4652195){$V-dim$}
\rput[bl](11.2,0.13478045){$y_i$}
\rput[bl](5.6,2.5347805){Hidden layer}
\rput[bl](9.2,2.9347804){Output layer}
\psdots[linecolor=black, dotsize=0.2](1.2571429,-2.8366482)
\psdots[linecolor=black, dotsize=0.2](1.2571429,-2.8366482)
\psdots[linecolor=black, dotsize=0.2](1.2571429,-2.8366482)
\psdots[linecolor=black, dotsize=0.2](1.2571429,-2.8366482)
\end{pspicture}
}
\caption{Mô hình CBOW}
\label{pic:cbow}
\end{figure}

Như đã nói, mô hình cbow dùng để dự đoán từ dựa vào ngữ cảnh cho trước.
Trong mô hình cbow, mỗi từ là một vector được thể hiện bằng cột trong ma trận $\textbf{W}$.
Mỗi giá trị trong cột là thứ tự của từ trong tập từ vựng.
Sự kết hợp của các vector được sử dụng như đặc trưng cho việc dự đoán từ xuất hiện tiếp theo trong câu.
Cho tập huấn luyện gồm các chữ như sau: $w_1, w_2, w_3, ..., w_T$, mục tiêu của mô hình word2vec này là tìm cách cực đại giá trị trung bình của xác suất logarit.
\begin{center}
\begin{equation}
\frac{1}{T} \sum_{t=k}^{T-k} log p(w_t|w_{t-k}, ..., w_{t+k})
\end{equation}
\end{center}

Quá trình dự đoán được thực hiện thông qua phân lớp đa lớp bằng việc sử dụng softmax.
\begin{center}
\begin{equation}
p(w_t|w_{t-k}, ..., w_{t+k}) = \frac{e^{y_{w_t}}}{\sum_i e^{y_i}}
\end{equation}
\end{center}

Mỗi $y_i$ là giá trị xác suất logarit chưa được chuẩn hóa cho đầu ra của từ thứ $i$, được tính bằng cách:
\begin{center}
\begin{equation}
y = b + Uh(w_{t-k}, ..., w_{t+k}; W)
\end{equation}
\label{med:cbow}
\end{center}

Công thức~\ref{med:cbow} có $U, b$ là tham số của softmax.
$h$ được tạo thành bằng liên kết hoặc trung bình hóa của các vector từ được trích xuất từ $W$.

Sau quá trình huấn luyện, những từ có ý nghĩa gần tương đồng sẽ được gắn vào vị trí tương ứng trong vector.
Ví dụ, từ ``powerful'' và từ ``strong'' có nghĩa gần tương đồng với nhau trong khi hai từ này lại khác xa so với từ ``Paris''.
Sự dị biệt về vector từ cũng đồng nghĩa với sự khác biệt về nghữ nghĩa.

Như vậy, cbow dựa vào những từ cho trước để rồi dự đoán từ có thể xuất hiện tiếp theo.
Quá trình để tạo thành vector cho văn bản cũng được thực hiện dựa trên cách thức tạo thành vector cho từ.
Mô hình dm của doc2vec cũng được thực hiện dựa trên mô hình cbow của word2vec.
Vector dùng để biểu diễn văn bản của mô hình này cũng thực hiện việc dự đoán từ xuất hiện tiếp theo dựa vào số lượng mẫu ngữ cảnh cho trước của các đoạn văn.

Hình~\ref{vd:cbow} cho ta thấy quá trình dự đoán từ dựa vào những từ khác.
Từ cách này, ta sẽ áp dụng cho toàn văn bản để cho ra được kết quả như hình~\ref{vd:dm}~\cite{doc2vec-original}.
Dựa theo hình~\ref{vd:dm}, mỗi đoạn văn được biểu diễn thành một vector, được thể hiện trong cột của ma trận $D$ và mỗi từ cũng được biểu diễn thành một vector, được thể hiện trong cột của ma trận $W$.
Vector của đoạn văn và từ được liên kết hoặc trung bình hóa để dự đoán từ có thể xuất hiện tiếp theo trong ngữ cảnh.

\begin{figure}[htp]
\psscalebox{1.0 1.0} % Change this value to rescale the drawing.
{
\begin{pspicture}(0,-2.92)(14.175,2.92)
\psdots[linecolor=black, dotstyle=square, dotsize=0.4, fillcolor=white](7.6,-0.52)
\psdots[linecolor=black, dotstyle=square, dotsize=0.4, fillcolor=white](8.0,-0.52)
\psdots[linecolor=black, dotstyle=square, dotsize=0.4, fillcolor=white](8.4,-0.52)
\psdots[linecolor=black, dotstyle=square, dotsize=0.4, fillcolor=white](8.8,-0.52)
\psdots[linecolor=black, dotstyle=square, dotsize=0.4, fillcolor=white](9.2,-0.52)
\psdots[linecolor=black, dotstyle=square, dotsize=0.4, fillcolor=white](10.0,-0.52)
\psdots[linecolor=black, dotstyle=square, dotsize=0.4, fillcolor=white](10.4,-0.52)
\psdots[linecolor=black, dotstyle=square, dotsize=0.4, fillcolor=white](10.8,-0.52)
\psdots[linecolor=black, dotstyle=square, dotsize=0.4, fillcolor=white](11.2,-0.52)
\psdots[linecolor=black, dotstyle=square, dotsize=0.4, fillcolor=white](11.6,-0.52)
\psdots[linecolor=black, dotstyle=square, dotsize=0.4, fillcolor=white](12.4,-0.52)
\psdots[linecolor=black, dotstyle=square, dotsize=0.4, fillcolor=white](12.8,-0.52)
\psdots[linecolor=black, dotstyle=square, dotsize=0.4, fillcolor=white](13.2,-0.52)
\psdots[linecolor=black, dotstyle=square, dotsize=0.4, fillcolor=white](13.6,-0.52)
\psdots[linecolor=black, dotstyle=square, dotsize=0.4, fillcolor=white](14.0,-0.52)
\psdots[linecolor=black, dotstyle=square, dotsize=0.4, fillcolor=white](9.6,1.08)
\psdots[linecolor=black, dotstyle=square, dotsize=0.4, fillcolor=white](10.0,1.08)
\psdots[linecolor=black, dotstyle=square, dotsize=0.4, fillcolor=white](10.4,1.08)
\psdots[linecolor=black, dotstyle=square, dotsize=0.4, fillcolor=white](10.8,1.08)
\rput[bl](8.0,-1.72){\textbf{W}}
\rput[bl](10.4,-1.72){\textbf{W}}
\rput[bl](12.8,-1.72){\textbf{W}}
\rput[bl](8.0,-2.92){the}
\rput[bl](10.4,-2.92){cat}
\rput[bl](12.8,-2.92){sat}
\rput[bl](10.0,2.68){on}
\psline[linecolor=black, linewidth=0.04, arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.0]{->}(8.4,-2.52)(8.4,-1.72)
\psline[linecolor=black, linewidth=0.04, arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.0]{->}(10.8,-2.52)(10.8,-1.72)
\psline[linecolor=black, linewidth=0.04, arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.0]{->}(13.2,-2.52)(13.2,-1.72)
\psline[linecolor=black, linewidth=0.04, arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.0]{->}(8.4,-1.32)(8.4,-0.92)
\psline[linecolor=black, linewidth=0.04, arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.0]{->}(10.8,-1.32)(10.8,-0.92)
\psline[linecolor=black, linewidth=0.04, arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.0]{->}(13.2,-1.32)(13.2,-0.92)
\psline[linecolor=black, linewidth=0.04, arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.0]{->}(8.4,-0.12)(9.6,0.68)
\psline[linecolor=black, linewidth=0.04, arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.0]{->}(10.8,-0.12)(10.4,0.68)
\psline[linecolor=black, linewidth=0.04, arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.0]{->}(13.2,-0.12)(11.2,0.68)
\psline[linecolor=black, linewidth=0.04, arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.0]{->}(10.0,1.48)(10.0,2.68)
\rput[bl](0.0,2.68){Classifier}
\rput[bl](0.0,1.08){Average/Concatenate}
\rput[bl](0.0,-1.72){Paragraph Matrix}
\psdots[linecolor=black, dotstyle=square, dotsize=0.4, fillcolor=white](5.2,-0.52)
\psdots[linecolor=black, dotstyle=square, dotsize=0.4, fillcolor=white](5.6,-0.52)
\psdots[linecolor=black, dotstyle=square, dotsize=0.4, fillcolor=white](6.0,-0.52)
\psdots[linecolor=black, dotstyle=square, dotsize=0.4, fillcolor=white](6.4,-0.52)
\psdots[linecolor=black, dotstyle=square, dotsize=0.4, fillcolor=white](6.8,-0.52)
\rput[bl](5.6,-1.72){\textbf{D}}
\rput[bl](4.8,-2.92){Paragraph id}
\psline[linecolor=black, linewidth=0.04, arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.0]{->}(6.0,-2.52)(6.0,-1.72)
\psline[linecolor=black, linewidth=0.04, arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.0]{->}(6.0,-1.32)(6.0,-0.92)
\psline[linecolor=black, linewidth=0.04, arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.0]{->}(6.0,-0.12)(9.2,0.68)
\psline[linecolor=black, linewidth=0.04, linestyle=dotted, dotsep=0.10583334cm, arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.0]{->}(2.8,-1.72)(5.6,-1.72)
\end{pspicture}
}
\caption{Quá trình dự đoán từ dựa vào ngữ cảnh văn bản}
\label{vd:dm}
\end{figure}

Sự thay đổi duy nhất trong mô hình dm của doc2vec so với mô hình cbow của word2vec trong công thức~\ref{med:cbow} là $h$ được tạo thành từ ma trận $W$ với $D$.
Biểu diễn của đoạn văn có thể được xem như là một từ khác.
Thuật toán sẽ hoạt động như thể là phần kí ức tìm kiếm lại những gì còn sót trong ngữ cảnh hiện tại.
Vì lý do này, tác giả đã gọi đây là mô hình \textit{``Distributed Memory Model of Paragraph Vectors (PV-DM)''}.

Ngữ cảnh của mô hình có chiều dài cố định và được lấy mẫu thông qua  đoạn văn.
Vector biểu diễn đoạn văn được chia sẻ cho tất cả ngữ cảnh được tạo thành từ cùng đoạn văn.
Trong khi đó, ma trận $W$ của vector từ được chia sẻ cho tất cả các đoạn văn.
Nghĩa là vector cho từ ``powerful'' là giống nhau trong mọi đoạn văn bản.

Quá trình huấn luyện cho vector đơạn văn bản và vector từ sử dụng xác suất ngẫu nhiên biến thiên giảm dần.
Độ biến thiên đạt được thông qua quá trình cập nhật lan truyền ngược.
Ở mỗi bước, dựa vào mẫu ngữ cảnh cố định từ đoạn văn bản ngẫu nhiên, thuật toán tính độ biến thiên lỗi từ mạng trong hình~\ref{vd:dm} và sử dụng độ biến thiên để cập nhật tham số trong mô hình.

Tại thời điểm dự đoán, thuật toán tính ra vector cho đoạn văn bản mới.
Điều này đạt được cũng từ sự biến thiên.
Trong bước này, các tham số còn lại trong mô hình bao gồm vector ma trận cho từ $W$ và các trọng số cho softmax đều được đều được giữ cố định.

Giả sử ta có $N$ đoạn văn bản trong ngữ liệu, $M$ từ trong tập từ vựng.
Ta muốn có được vector cho đoạn văn bản với mỗi đoạn văn bản có $p$ chiều và mỗi vector từ có $q$ chiều thì ta sẽ có $N * p + M * q$ tham số.
Mặc dù số lương tham số có thể lớn nếu $N$ lớn nhưng quá trình cập nhật trong lúc huấn luyện diễn ra ít nên vì thế mô hình này được xem là hiệu quả.

Sau khi huấn luyện, vector cho đoạn văn bản có thể được sử dụng như đặc trưng của đoạn văn bản đó.
Ta có thể sử dụng vector này cho các thuật toán trong máy học để có thể đạt được kết quả cao.
Đồ án sử dụng vector văn bản được tạo thành dựa vào ý tưởng trên để tiến hành gom nhóm phân cấp văn bản.

Tóm gọn lại, thuật toán dành cho mô hình dm có 2 ý chính.
Thứ nhất, ta sẽ huấn luyện để lấy được vector ma trận từ $W$, các trọng số $U$ và $b$ trong softmax và vector ma trận văn bản $D$.
Thứ hai, tại bước dự đoán để có được vector cho đoạn văn bản mới từ ma trận $D$, thuật toán thêm nhiều cột và biến thiên vào trong $D$ trong khi $W, U$ và $b$ được giữ nguyên.
%Ngoài ra, ta sử dụng $D$ để dự đoán một vài nhãn bằng phương pháp phân loại chuẩn.

\subsection{Mô hình distributed-bag-of-words (dbow)}
Mô hình dbow của doc2vec tương đồng với mô hình sg của word2vec.
Mô hình sg được xem như là phần đối nghịch của cbow trong word2vec.
Trong cbow, mô hình dự đơán một chữ từ ngữ cảnh cho trước nhưng trong sg thì điều này ngược lại, nghĩa là ta sẽ sử dụng 1 chữ để dự đoán các chữ xung quanh chữ này.
Hình~\ref{pic:sg}~\cite{word2vec-sg} thể hiện mô hình sg trong word2vec.

Như đã nói, mô hình sg là sự đối nghịch với mô hình cbow trong word2vec.
Mô hình sg~\ref{pic:sg} thể hiện rõ điều đó khi so sánh với mô hình cbow~\ref{pic:cbow}.
Tượng tự như cbow, mạng nơ-ron của sg cũng có 3 lớp.
Lớp đầu vào là một vector chữ theo one-hot và lớp đầu ra là tập vector one-hot $y_1, ..., y_C$.
Ma trận trọng số $W$ có kích thước là $V * N$ (với $N$ được khởi tạo ngẫu nhiên), kết nối lớp đầu vào với lớp ẩn và mỗi dòng trong $W$ tương ứng với trọng số của từ thứ $i$ trong tập từ vựng.
Ngoài ra, ma trận trọng số $W$ chứa vector mã hóa của tất cả các từ trong tập từ vựng.
Lớp xuất là tập các vector từ mà mỗi vector là sự kết hợp của ma trận $W'$ (có kích thước $N * V$) với lớp ẩn.

\begin{figure}[htp]
\psscalebox{1.0 1.0} % Change this value to rescale the drawing.
{
\begin{pspicture}(0,-9.5183525)(13.16,9.5183525)
\pscircle[linecolor=black, linewidth=0.04, dimen=outer](10.057143,8.710219){0.4}
\pscircle[linecolor=black, linewidth=0.04, dimen=outer](10.057143,7.510219){0.4}
\psdots[linecolor=black, dotsize=0.2](10.057143,6.3102193)
\psdots[linecolor=black, dotsize=0.2](10.057143,5.510219)
\pscircle[linecolor=black, linewidth=0.04, dimen=outer](10.057143,4.7102194){0.4}
\pscircle[linecolor=black, linewidth=0.04, dimen=outer](2.057143,2.3102193){0.4}
\pscircle[linecolor=black, linewidth=0.04, dimen=outer](2.057143,1.1102192){0.4}
\psdots[linecolor=black, dotsize=0.2](2.057143,-0.08978072)
\psdots[linecolor=black, dotsize=0.2](2.057143,-0.8897807)
\pscircle[linecolor=black, linewidth=0.04, dimen=outer](2.057143,-1.6897807){0.4}
\pscircle[linecolor=black, linewidth=0.04, dimen=outer](10.057143,-4.089781){0.4}
\pscircle[linecolor=black, linewidth=0.04, dimen=outer](10.057143,-5.2897806){0.4}
\psdots[linecolor=black, dotsize=0.2](10.057143,-6.489781)
\psdots[linecolor=black, dotsize=0.2](10.057143,-7.2897806)
\pscircle[linecolor=black, linewidth=0.04, dimen=outer](10.057143,-8.089781){0.4}
\psdots[linecolor=black, dotsize=0.2](10.857142,-2.8897808)
\psdots[linecolor=black, dotsize=0.2](10.857142,-3.2897806)
\pscircle[linecolor=black, linewidth=0.04, dimen=outer](6.0571427,1.5102193){0.4}
\psdots[linecolor=black, dotsize=0.2](6.0571427,0.71021926)
\psdots[linecolor=black, dotsize=0.2](6.0571427,-0.08978072)
\pscircle[linecolor=black, linewidth=0.04, dimen=outer](6.0571427,-0.8897807){0.4}
\psframe[linecolor=black, linewidth=0.04, dimen=outer](10.457143,9.51022)(9.657143,3.9102192)
\psframe[linecolor=black, linewidth=0.04, dimen=outer](2.4571428,3.1102192)(1.6571429,-2.4897807)
\psframe[linecolor=black, linewidth=0.04, dimen=outer](10.457143,-3.2897806)(9.657143,-8.889781)
\pscircle[linecolor=black, linewidth=0.04, dimen=outer](10.057143,2.3102193){0.4}
\pscircle[linecolor=black, linewidth=0.04, dimen=outer](10.057143,1.1102192){0.4}
\psdots[linecolor=black, dotsize=0.2](10.057143,-0.08978072)
\psdots[linecolor=black, dotsize=0.2](10.057143,-0.8897807)
\pscircle[linecolor=black, linewidth=0.04, dimen=outer](10.057143,-1.6897807){0.4}
\psframe[linecolor=black, linewidth=0.04, dimen=outer](10.457143,3.1102192)(9.657143,-2.4897807)
\psframe[linecolor=black, linewidth=0.04, dimen=outer](6.457143,2.3102193)(5.6571426,-1.6897807)
\psline[linecolor=black, linewidth=0.04](2.4571428,3.1102192)(5.6571426,2.3102193)
\psline[linecolor=black, linewidth=0.04](2.4571428,-2.4897807)(5.6571426,-1.6897807)
\psline[linecolor=black, linewidth=0.04](6.457143,2.3102193)(9.657143,3.1102192)
\psline[linecolor=black, linewidth=0.04](6.457143,-1.6897807)(9.657143,-2.4897807)
\rput[bl](0.0,0.8816479){$x_{k}$}
\rput[bl](1.2,3.6816478){Input layer}
\rput[bl](2.8,0.08164786){$\textbf{W}_{V*N}$}
\rput[bl](9.2,-9.5183525){$C*V-dim$}
\rput[bl](4.0,-2.718352){$N-dim$}
\rput[bl](4.8,0.08164786){$h_i$}
\rput[bl](8.0,0.48164785){$\textbf{W'}_{N*V}$}
\rput[bl](1.6,-3.1183522){$V-dim$}
\rput[bl](11.2,0.48164785){$y_{2j}$}
\rput[bl](3.6,2.8816478){Hidden layer}
\rput[bl](11.2,8.881648){Output layer}
\psdots[linecolor=black, dotsize=0.2](10.857142,-2.4897807)
\psdots[linecolor=black, dotsize=0.2](10.857142,-2.4897807)
\psdots[linecolor=black, dotsize=0.2](10.857142,-2.4897807)
\psdots[linecolor=black, dotsize=0.2](10.857142,-2.4897807)
\psline[linecolor=black, linewidth=0.04](6.457143,2.3102193)(9.657143,9.51022)
\psline[linecolor=black, linewidth=0.04](9.657143,3.9102192)(6.457143,-1.6897807)
\psline[linecolor=black, linewidth=0.04](6.457143,2.3102193)(9.657143,-3.2897806)
\psline[linecolor=black, linewidth=0.04](6.457143,-1.6897807)(9.657143,-8.889781)
\rput[bl](8.0,4.481648){$\textbf{W'}_{N*V}$}
\rput[bl](8.0,-3.9183521){$\textbf{W'}_{N*V}$}
\rput[bl](11.2,6.481648){$y_{1j}$}
\rput[bl](11.2,-5.918352){$y_{Cj}$}
\end{pspicture}
}
\caption{Mô hình sg}
\label{pic:sg}
\end{figure}

Hình~\ref{pic:sg} cho thấy được mô hình sg đối nghịch đối với cbow trong word2vec.
Tương tự, mô hình dm và dbow cũng có mối quan hệ giống như vậy.
Mô hình dm dựa vào ngữ cảnh của những từ đầu vào để dự đoán từ xuất hiện tiếp theo.
Trong khi đó, dbow loại bỏ ngữ cảnh từ trong đầu vào và dự đoán ngẫu nhiên những từ trong đoạn văn ở lớp đầu ra.
Trong thực tế, điều này có nghĩa là ở mỗi vòng lặp của độ xác suất biến thiên giảm dần, ta lấy mẫu khung văn bản, rồi từ khung văn bản này chọn ra từ ngẫu nhiên và tạo ra nhiệm vụ phân lớp dựa vào vector của đoạn văn bản.
Kỹ thuật này được thể hiện trong ví dụ~\ref{vd:dbow}~\cite{doc2vec-original}.
Nhóm tác giả đã đặt tên phiên bản này là Distributed Bag of Words version of Paragraph Vector (PV-DBOW).

\begin{figure}[htp]
\psscalebox{1.0 1.0} % Change this value to rescale the drawing.
{
\begin{pspicture}(0,-2.92)(9.56,2.92)
\psdots[linecolor=black, fillstyle=solid, dotstyle=square, dotsize=0.4, fillcolor=white](6.8,1.08)
\psdots[linecolor=black, fillstyle=solid, dotstyle=square, dotsize=0.4, fillcolor=white](7.2,1.08)
\psdots[linecolor=black, fillstyle=solid, dotstyle=square, dotsize=0.4, fillcolor=white](7.6,1.08)
\psdots[linecolor=black, fillstyle=solid, dotstyle=square, dotsize=0.4, fillcolor=white](8.0,1.08)
\rput[bl](3.6,2.68){the}
\rput[bl](5.6,2.68){cat}
\rput[bl](7.6,2.68){sat}
\rput[bl](9.2,2.68){on}
\rput[bl](1.2,2.68){Classifier}
\rput[bl](0.0,-1.72){Paragraph Matrix}
\rput[bl](7.2,-1.72){\textbf{D}}
\rput[bl](6.4,-2.92){Paragraph id}
\psline[linecolor=black, linewidth=0.04, linestyle=dotted, dotsep=0.10583334cm, arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.0]{->}(2.8,-1.72)(6.8,-1.72)
\psline[linecolor=black, linewidth=0.04](7.2,-2.52)(7.2,-1.72)
\psline[linecolor=black, linewidth=0.04, arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.0]{->}(7.2,-1.32)(7.2,0.68)
\psline[linecolor=black, linewidth=0.04, arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.0]{->}(6.8,1.48)(4.0,2.68)
\psline[linecolor=black, linewidth=0.04, arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.0]{->}(7.2,1.48)(6.0,2.68)
\psline[linecolor=black, linewidth=0.04, arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.0]{->}(7.6,1.48)(7.6,2.68)
\psline[linecolor=black, linewidth=0.04, arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.0]{->}(8.0,1.48)(9.2,2.68)
\end{pspicture}
}
\caption{Quá trình dự đoán của dbow}
\label{vd:dbow}
\end{figure}

\section{Gom nhóm phân cấp kết hợp với văn bản doc2vec}
Như đã giới thiệu, doc2vec là thuật toán học không giám sát để tạo ra vector thể hiện cho câu, đoạn văn hay là văn bản.
Thuật toán doc2vec cùng là phần chỉnh sửa, nâng cấp từ word2vec.
Đồ án sử dụng thuật toán doc2vec từ thư viện gensim\ref{gensim} để huấn luyện tập dữ liệu.
Vì doc2vec là phần mở rộng từ word2vec nên có thể tái sử dụng lại những mẫu từ word2vec. 
Ta có thể dễ dàng chỉnh sửa lại chiều của vector, kích thước của tham số \textit{sliding window}, số lượng \textit{workers} hay là bất kì tham số nào tương ứng với việc thay đổi trong mô hình Word2vec.

Điều duy nhất khác biệt giữa thuật toán doc2vec với word2vec là liên quan đến phương pháp huấn luyện khi sử dụng mô hình doc2vec.
Trong kiến trúc word2vec, hai thuật toán được sử dụng là continuous bag of word (cbow) và skip-gram (sg).
Đối với doc2vec, thuật toán tương ứng là distributed memory (dm) và distributed bags of words (dbow).
Vì mô hình dm có hiệu năng tốt hơn đây sẽ là thuật toán mặc định lúc chạy doc2vec.
Tuy nhiên, ta vẫn có thể chuyển sang mô hình dbow nếu muốn bằng việc thiết lập cờ \textit{dm=0} trong hàm khởi tạo.

Phần dữ liệu đầu vào của doc2vec là một dãy những đối tượng \textbf{TaggedDocument} với mỗi đối tượng thể hiện văn bản và có 2 phần cơ bản là tập hợp các từ và nhãn như sau:
\begin{lstlisting}[language=Python]
taggedDocument = TaggedDocument(words=[u'some', u'words', u'here'], labels=[u'SENT_1'])
\end{lstlisting}

Thuật toán sẽ chạy qua \textbf{TaggedDocument} 2 lần lặp.
Lần đầu để xây dựng tập từ, lần thứ hai để huấn luyện mô hình cho tập dữ liệu đầu vào, học cách thể hiện vector cho mỗi từ và nhãn cho ngữ liệu.
Đồ án sẽ thực thi một lớp để tạo ra những vector cho tập văn bản.
\begin{lstlisting}[language=Python]
class DocIterator:
    def __init__(self, doclist, labellist):
        self.DocList = doclist
        self.LabelList = labellist

    def __iter__(self):
        for idx, doc in enumerate(self.DocList):
            yield gensim.models.doc2vec.TaggedDocument(doc.split(), [self.LabelList[idx]])
\end{lstlisting}

Doc2vec sẽ học cách thể hiện đồng thời cho từ và nhãn.
Một điều lưu ý là quá trình huấn luyện của doc2vec có tỷ lệ học giảm dần qua tiến trình lặp qua dữ liệu, nhãn.
Để khắc phục điều này, tỷ lệ học cần phải được điều chỉnh một cách thủ công và được thực thi trong đồ án như sau:
\label{code:training}
\begin{lstlisting}[language=Python]
model = Doc2Vec(size=100, window=10, min_count=1, workers=4, alpha=0.025, min_alpha=0.025)
    model.build_vocab(documents)
    for epoch in range(10):
        model.train(documents)
        model.alpha -= 0.002
        model.min_alpha = model.alpha
\end{lstlisting}

Ý nghĩa của các tham số của doc2vec trong phần mã nguồn trên:
\begin{enumerate}
\item[•]$documents$: là dãy bao gồm các phần tử TaggedDocuments.
\item[•]$dm$: cách huấn luyện mặc định, có giá trị mặc định là 1.
\item[•]$size$: số chiều được thiết lập cho vector.
\item[•]$window$: là khoảng cách tối đa giữa từ được dự đoán và từ được sử dụng trong ngữ cảnh dự đoán trong cùng một văn bản.
\item[•]$alpha$: giá trị khởi tạo cho tỷ lệ học.
\item[•]$workers$:  số lượng luồng được sử dụng để huấn luyện doc2vec.
\end{enumerate}

Sau khi huấn luyện xong, ta có thể trích xuất vector của văn bản dựa vào nhãn của từng văn bản.
\begin{lstlisting}[language=Python]
model.docvecs[label]
\end{lstlisting}

Đồ án sẽ huấn luyện doc2vec cho tập dữ liệu tiếng Anh \ref{sec:dlta} và tiếng Việt \ref{sec:dltv}.
Trong đó, tập dữ liệu tiếng Việt sẽ bao gồm 2 phần: tập huấn luyện và tập kiểm nghiệm kết quả.
Khi huấn luyện doc2vec cho phần dữ liệu tiếng Việt, ta sẽ trộn 2 tập này thành dữ liệu đầu vào của thuật toán.
Các tham số được sử dụng cũng như là giá trị của một số tham số sẽ được thiết lập như trong phần mã nguồn \ref{code:training}.
Sau đó, đồ án sẽ trích xuất những vector từ mô hình doc2vec dựa vào nhãn của văn bản, riêng đối với dữ liệu tiếng Việt thì chỉ trích xuất từ tập kiểm nghiệm kết quả.
Cuối cùng, những thể hiện vector có số chiều là 100 sẽ được đem đi gom nhóm phân cấp kết hợp.

Thuật toán gom nhóm phân cấp kết hợp sử dụng trong đồ án đến từ thư viện scikit-learn~\cite{hac-scikit}.

\begin{lstlisting}[language=Python]
AgglomerativeClustering(n_clusters=2, affinity=euclidean, memory=None, connectivity=None, compute_full_tree=auto, linkage=ward, pooling_func=<function mean>)
\end{lstlisting}
\label{ahc-scikit}
%\textit{AgglomerativeClustering(n{\_}clusters=2, affinity=`euclidean', \\
%memory=None, connectivity=None, compute{\_}full{\_}tree=`auto', \\
%linkage=`ward', pooling{\_}func=<function mean>)}

Ý nghĩa của các tham số trong thuật toán gom nhóm phân cấp kết hợp:
\begin{enumerate}
\item[•]$n{\_}clusters$: số lượng nhóm cần phải tìm, có giá trị mặc định là 2.
\item[•]$affinity$: công thức để tính khoảng cách giữa 2 văn bản, có nhiều công thức khác nhau: ``euclidean'', ``l1'', ``l2'', ``manhattan'', ``cosine'' và ``precomputed''.
Trong trường hợp cách liên kết ``ward'' được sử dụng thì chỉ có thể kết hợp với công thức tính khoảng cách ``euclidean''.
\item[•]$memory$: thể hiện của lớp sklearn.externals.joblib.Memory hoặc là chuỗi, có giá trị mặc định là \textit{None}.
Memory được sử dụng để lưu trữ cục bộ giá trị đầu ra của cây phân cấp.
Mặc định thì giá trị này không được lưu.
Trong trường hợp giá trị của memory là \textit{string} thì chính là đường dẫn đến thư mục lưu trữ cục bộ.
\item[•]$connectivity$: tham số tùy chọn, là tham số dạng mảng hoặc là hàm.
Connectivity matrix là ma trận kết nối dùng để định nghĩa mỗi mẫu thì có mẫu lân cận theo cấu trúc dữ liệu có sẵn.
Ma trận kết nối có thể là chính bản thân nó hoặc là dùng để chuyển hóa dữ liệu vào nó như dẫn xuất từ kneighbors{\_}graph.
Giá trị mặc định của tham số này là \textit{None}.
\item[•]$compute{\_}full{\_}tree$: có giá trị bool hoặc là ``auto'' và là tham số tùy chọn.
Tham số này có tác dụng kết thúc sớm việc tạo cây phân cấp tại nhóm thứ \textit{n{\_}clusters}.
Điều này hữu dụng với việc giảm thời gian tính toán và nếu kết hợp với tham số ma trận kết nối.
\item[•]$linkage$: cách thức liên kết trong gom nhóm phân cấp kết hợp với các tùy chọn như sau: {``ward'', ``complete'', ``average''} và cách liên kết mặc định là ``ward''.
\item[•]$pooling{\_}func$: hàm, có giá trị mặc định là \textit{np.mean}.
Tham số này kết hợp giá trị của đặc trưng tích tụ thành giá trị duy nhất.
Ta có mảng [M, N] và tham số này là \textit{axis=1} thì mảng sẽ giảm xuống còn [M].
\end{enumerate}

Đồ án sử dụng gom nhóm phân cấp kết hợp như đã nêu ở phần \textit{AgglomerativeClustering} \ref{ahc-scikit} với các tham số $n{\_}clusters$, $linkage$, $affinity$.
Trong đó, giá trị $n{\_}clusters$ sẽ được thiết lập tùy vào dữ liệu đầu vào.
Tiếp đến, đồ án sẽ tiến hành gom nhóm phân cấp kết hợp cho cả 2 mô hình doc2vec và TF-IDF.
Sau khi kết thúc gom nhóm, đồ án sử dụng 2 chỉ số \ref{sec:NMI} và \ref{sec:ARI} để so sánh kết quả gom nhóm của doc2vec với TFIDF.
Chi tiết phần thực nghiệm sẽ được nói rõ ở phần \ref{Chapter4}.
%Dựa vào phân tích về tốc độ thực thi cũng như là ảnh hưởng đến lưu trữ bộ nhớ của gom nhóm phân cấp, ta có thể tiến hành cải thiện thuật toán dựa vào 2 hướng tiếp cận này.
%Đồ án tập trung vào cải thiện bộ nhớ lưu trữ trong quá trình gom nhóm phân cấp.
%Như đã đề cập ở trên, dung lượng cần thiết để chạy chương trình là $O(m^2)$, với m là số lượng điểm trong dữ liệu.
%Tuy nhiên, các thực nghiệm chạy trong các ví dụ trên chỉ gồm các điểm có 2 chiều.
%Nhưng trên thức tế, các văn bản để gom nhóm thường rất lớn, nên dung lương lúc chạy thuật toán là rất lớn.
%
%Mục tiêu của đồ án là gom nhóm văn bản tin tức tiếng Việt nên ngữ liệu sẽ rất lớn.
%Do văn bản là từ ngữ sẽ gây khó khăn khi tính toán nên ta sẽ phải chuyển đồi thể hiện từ từ ngữ sang số.
%Ta sẽ chuyển đổi mỗi văn bản thành định dạng vector, với chiều của vector tương ứng với số lượng từ ngữ trong ngữ liệu.
%Điều này đồng nghĩa chiều của vector sẽ phụ thuộc vào số lương từ có trong ngữ liệu.
%Nếu số lượng từ càng nhiều thì chiều của văn bản càng lớn, điều này dẫn lớn lúc chạy thuật toán gom nhóm phân cấp thì dung lượng sẽ rất lớn.
%
%Việc sử dụng thể hiện của văn bản với vector có số chiều tương ứng với số lượng từ trong ngữ liệu sẽ khiến cho thuật toán bị giới hạn do tốn quá nhiều dung lượng bộ nhớ.
%Vì vậy, ta có thể thay đổi thể hiện của văn bản chuyển từ sử dụng tần số sang doc2vec.
%Doc2vec là thuật toán không giám sát để tạo ra vector cho câu, đoạn văn hoặc là văn bản.
%Thuật toán là phiên bản tương thích với word2vec, dùng để tạo ra vector cho từ.
%
%Vector được tạo bởi doc2vec thường được sử dụng cho các nhiệm vụ như tìm độ tương đồng giữa câu, đoạn văn và văn bản.
%Không như các mô hình câu như RNN, các chuỗi từ được giữ lại trong quá trình tạo ra vector câu, doc2vec độc lập với thứ tự từ.
%Trong nhiệm vụ tìm kiếm độ tương đồng, doc2vec biểu diễn định dạng của văn bản rất tốt vì giới hạn được số lượng chiều.
%Vì vậy, doc2vec được chọn để biểu diễn văn bản cho thuật toán gom nhóm phân cấp.
%
%Do vector biểu diễn bằng doc2vec có số chiều nhỏ hơn rất nhiều so với vector biểu diễn tần số, dung lượng bộ nhớ khi sử dụng thuật toán gom nhóm phân cấp sẽ được giảm đáng kể.
%Vector biểu diễn bằng tần số có số chiều tương ứng với số lượng từ trong ngữ liệu.
%Trong khi đó, số chiều trong doc2vec được thiết lập cố định.
%Vì doc2vec là thuật toán dùng để tìm kiếm thể hiện cho văn bản nên ta phải thiết lập cố định số chiều lúc huấn luyện.
%Chính vì điều này nên doc2vec có số chiều nhỏ hơn so với tần số và sẽ giúp cho dung lượng bộ nhớ giảm đi nhiều khi thực thi chương trình gom nhóm phân cấp.

%Đây là hướng tiếp cận kinh điển trong việc thể hiện văn bản, có tên gọi là TFIDF (term-frequecny - inverse frequency document).
%TFIDF là một dạng thống kê số học 



%Phân tích điểm hạn chế của thuật toán(2-3 đoạn)

%Biến liên tục

%Biến rời rạc

%Độ đo kết hợp giữa hai biến rời rạc

%Các độ đo kết hợp cho biến rời rạc(2-3)

%Độ đo kết hợp giữa hai biến liên tục

%Các độ đo kêt hợp cho biến liên tục(2-3)

%Goodman kruskal chỉ áp dụng cho biến rời rạc

%Tìm cách cải thiện thuật toán(1 - 2)

%Sử dụng doc2vec cho thể hiện văn bản

%Tìm kiếm công thức tính khoảng cách tốt nhất có thể (8-10)

%Áp dụng vào thuật toán hiện hành

%kết quả

%%23-29